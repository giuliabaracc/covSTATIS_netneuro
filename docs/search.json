[
  {
    "objectID": "pages/tutorial.html",
    "href": "pages/tutorial.html",
    "title": "Apply regular covSTATIS to task fMRI data",
    "section": "",
    "text": "All data used in this tutorial are stored here. The “Data” repo contains: 144x3 functional connectivity text files (n=144, 3=task conditions, 0-,1-,2-back), a “participants_ages.csv” file with participants info and an atlas labels file “Schaefer_100parcel_17network_labels.csv”."
  },
  {
    "objectID": "pages/tutorial.html#setup",
    "href": "pages/tutorial.html#setup",
    "title": "Apply regular covSTATIS to task fMRI data",
    "section": "",
    "text": "All data used in this tutorial are stored here. The “Data” repo contains: 144x3 functional connectivity text files (n=144, 3=task conditions, 0-,1-,2-back), a “participants_ages.csv” file with participants info and an atlas labels file “Schaefer_100parcel_17network_labels.csv”."
  },
  {
    "objectID": "pages/tutorial.html#data-info",
    "href": "pages/tutorial.html#data-info",
    "title": "Apply regular covSTATIS to task fMRI data",
    "section": "Data info",
    "text": "Data info\nAfter standard preprocessing (see Rieck et al., 2021 Data in Brief), each individual’s voxel-time series were parcellated, for each task condition, using the standard 100 region-17 network Schaefer atlas (Schaefer et al., 2018 Cerebral Cortex). For each participant and condition, Pearson’s correlations were quantified between region pairs, to ultimately obtain 100x100x3 functional connectivity matrices for each individual."
  },
  {
    "objectID": "pages/tutorial.html#read-in-the-data",
    "href": "pages/tutorial.html#read-in-the-data",
    "title": "Apply regular covSTATIS to task fMRI data",
    "section": "Read in the data",
    "text": "Read in the data\nThe first step is to read in our connectivity matrices to form a 3D array (= ‘X’ here below) for analysis with covSTATIS. In this tutorial, the 3D array dimensionality is 100 x 100 x 144 x 3 (ROI x ROI x N x Task condition).\n\natlas_prefix &lt;- \"Schaefer100_17\"\natlas_labels &lt;- read_csv('../Data/Schaefer_100parcel_17network_labels.csv')\natlas_labels$Network &lt;- as.factor(atlas_labels$Network)\ndemog_in &lt;- read_csv('../Data/participant_ages.csv')\nids &lt;- demog_in$subjectID\n\n## Read in FC .txt files and reshape into 3D array with size n_roi x n_roi x n_sub*n_conditions (i.e. n_fns)\nall_fns&lt;-list.files(path = '../Data', pattern = paste0('^', atlas_prefix, '.*txt'), full.names = TRUE)\nX_list&lt;-lapply(X=all_fns, FUN=read_delim, delim=\"\\t\", col_names = F,col_type = list(.default = col_double()))\nX &lt;- array(unlist(X_list), c(dim(X_list[[1]]), length(X_list))) #our 3D array\nrm(X_list) \n\n## Label rows and columns with ROI label\nif(length(grep(\"Schaefer\", atlas_prefix)&gt;0)){\n  rownames(X)&lt;- atlas_labels$ROI_label\n  colnames(X)&lt;- atlas_labels$ROI_label\n}\n\n## Label 3rd dimension with subject and task information (modified from filenames)\ndimnames(X)[[3]]&lt;-all_fns %&gt;% gsub(pattern=\".txt\", replacement = \"\") %&gt;%\n  gsub(pattern=paste0(atlas_prefix,\"_\"), replacement = \"\")"
  },
  {
    "objectID": "pages/tutorial.html#run-covstatis",
    "href": "pages/tutorial.html#run-covstatis",
    "title": "Apply regular covSTATIS to task fMRI data",
    "section": "Run covSTATIS",
    "text": "Run covSTATIS\nNow that we have created our 3D data array (X), we can run covSTATIS. Since we are dealing with correlation values, the Distance flag should be set to FALSE. The nfact2keep flag denotes the number of components we want to keep (the bigger the value, the slower the computations).\n\ncovstatis_res &lt;- distatis(X, Distance = F,nfact2keep = 10,compact = F)\n\n\nStep 1: RV similarity matrix\nThe first operation covSTATIS computes is the generation of an RV similarity matrix. The RV matrix quantifies the pairwise similarity among all data tables (squared Pearson’s correlation), irrespective of their rotation or scaling. The RV matrix then undergoes eigenvalue decomposition (EVD). The resulting first component best represents the common pattern across all tables, and its first eigenvector quantifies how similar each table is to this common pattern.\nLet’s visualize the results of this EVD on the RV matrix. The Scree plot shows how much variance is explained by each component/dimension. Ideally, there will be one strong component (as is the case with these data) indicating a strong coherence in the general network organization across all observations.\n\nPlotScree(ev = covstatis_res$res4Cmat$eigValues, \n          title = \"RV-map: Explained Variance per Dimension\")\n\n\n\n\nThe factor map of the RV space shows how similar the data tables are to each other.\n\nrv.labels &lt;- createxyLabels.gen(x_axis = 1,\n                                y_axis = 2,\n                                lambda = covstatis_res$res4Cmat$eigValues,\n                                tau = covstatis_res$res4Cmat$tau)\n## factor map\nrv.map &lt;- createFactorMap(covstatis_res$res4Cmat$G,\n                          title = \"The component space of the RV matrix\",\n                          col.background = NULL, col.axes = \"#42376B\",width.axes = 1, alpha.axes = 0.5)\n\nrv.map$zeMap_background + rv.map$zeMap_dots + rv.labels + scatter.theme\n\n\n\n\n\n\nStep 2: Compromise space\nNext, covSTATIS grabs the first eigenvector of this EVD on RV and scales it to sum to 1. The compromise matrix is built from the weighted sum of all data matrices and then also submitted to EVD. Let’s first visualize the compromise space/matrix.\n\nnet.col &lt;- as.data.frame(unique(atlas_labels[,c(3,4)]))\nrownames(net.col) &lt;- net.col$Network\n\ncol.pal &lt;- brewer.pal(11, \"RdBu\")\nsuperheat(covstatis_res$res4Splus$Splus,\n          membership.cols = atlas_labels$Network,\n          membership.rows = atlas_labels$Network,\n          clustering.method = NULL,\n          heat.lim = c(-0.1,0.1),\n          heat.pal = rev(brewer.pal(11, \"RdBu\")),\n          heat.pal.values = c(0,0.45,0.5,0.55,1),\n          left.label.size = 0.08,\n          bottom.label.size = 0.05,\n          y.axis.reverse = TRUE,\n          left.label.col = net.col[levels(atlas_labels$Network),\"Network_color\"], # order by community name\n          bottom.label.col =  net.col[levels(atlas_labels$Network), \"Network_color\"],\n          left.label.text.size = 3,\n          bottom.label.text.size = 2,\n          left.label.text.col = \"black\",\n          bottom.label.text.col = \"black\",\n          left.label.text.alignment = \"left\",\n          title = \"The compromise\")$plot\n\n\n\n\nTableGrob (7 x 4) \"layout\": 5 grobs\n  z     cells   name                     grob\n1 1 (3-3,3-3)  panel gTree[panel-1.gTree.106]\n2 2 (6-6,3-3) layout           gtable[layout]\n3 3 (3-3,2-2) layout           gtable[layout]\n4 4 (4-4,3-3) layout           gtable[layout]\n5 5 (2-2,3-3) layout           gtable[layout]\n\n\nNext, the compromise undergoes EVD. Let’s visualize its scree plot:\n\nPlotScree(ev = covstatis_res$res4Splus$eigValues,\n          title = \"Compromise: Explained Variance per Dimension\")\n\n\n\n\nLet’s now reconstruct a heatmap to explore the dimensions from the EVD on the compromise. Let’s start by visualizing the first two components (LV1, LV2).\n\ncovstatis_corrmat_lv1_lv2 &lt;- tcrossprod(covstatis_res$res4Splus$F[,c(1,2)])\n\nsuperheat(covstatis_corrmat_lv1_lv2,\n          membership.cols = atlas_labels$Network,\n          membership.rows = atlas_labels$Network,\n          clustering.method = NULL,\n          heat.lim = c(-0.1,0.1),\n          heat.pal = rev(brewer.pal(11, \"RdBu\")),\n          heat.pal.values = c(0,0.45,0.5,0.55,1),\n          left.label.size = 0.08,\n          bottom.label.size = 0.05,\n          y.axis.reverse = TRUE,\n          left.label.col = net.col[levels(atlas_labels$Network),\"Network_color\"], # order by community name\n          bottom.label.col =  net.col[levels(atlas_labels$Network), \"Network_color\"],\n          left.label.text.size = 3,\n          bottom.label.text.size = 2,\n          left.label.text.col = \"black\",\n          bottom.label.text.col = \"black\",\n          left.label.text.alignment = \"left\",\n          title = \"Rebuilt Heatmap for LV1 and LV2\")$plot\n\n\n\n\nTableGrob (7 x 4) \"layout\": 5 grobs\n  z     cells   name                     grob\n1 1 (3-3,3-3)  panel gTree[panel-1.gTree.314]\n2 2 (6-6,3-3) layout           gtable[layout]\n3 3 (3-3,2-2) layout           gtable[layout]\n4 4 (4-4,3-3) layout           gtable[layout]\n5 5 (2-2,3-3) layout           gtable[layout]\n\n\n\n\nStep 3: global factor scores from compromise space\nThe compromise space shows us how regional connectivity values relate to each other across all data tables (individuals and conditions). Each point in the compromise space represents a global factor score: a different ROI from our functional connectivity matrices, colored by their network membership. The closer points are, the stronger the functional connectivity across the entire dataset.\n\nnetwork_labels &lt;- unique(atlas_labels$Network)\nnetwork_colors &lt;- unique(atlas_labels$Network_color)\n\n# To get graphs with axes 1 and 2:\nh_axis &lt;-1 # component to plot on x-axis\nv_axis &lt;-2 # component to plot on y-axis\n\ncompromise_graph_out &lt;- createFactorMap(covstatis_res$res4Splus$F,\n                                        axis1 = h_axis, axis2 = v_axis,\n                                        title = 'Compromise ROI Space',\n                                        col.points = atlas_labels$Network_color,\n                                        alpha.points=.6, cex=3,\n                                        col.background = NULL, col.axes = \"#42376B\",width.axes = 1, alpha.axes = 0.5) \nprint(compromise_graph_out$zeMap_background + compromise_graph_out$zeMap_dots + scatter.theme)\n\n\n\n\nWe can also compute the average of all the ROIs (i.e., barycenter) for each a priori network to get a coarser network compromise map.\n\nnetwork_mean_F &lt;- t(apply(makeNominalData(as.matrix(atlas_labels$Network)),2,function(x){x/sum(x)})) %*% covstatis_res$res4Splus$F\nrow.names(network_mean_F) &lt;- network_labels\n\ncompromise_network_graph_out &lt;- createFactorMap(network_mean_F,\n                                                axis1 = h_axis,axis2 = v_axis, \n                                                title = \"Compromise map for network-level barycenters\",\n                                                col.points = network_colors, col.labels = network_colors,\n                                                alpha.points=.8, cex=5, text.cex=4, alpha.labels=.8, pch=18,\n                                                col.background = NULL, col.axes = \"#42376B\",width.axes = 1, alpha.axes = 0.5)\nprint(compromise_network_graph_out$zeMap + compromise_network_graph_out$zeMap_dots + scatter.theme)\n\n\n\n\nBoth the ROI-level and network-level factor maps can be overlaid to show how individuals ROIs cluster around their respective networks, for the whole sample.\n\nprint(compromise_graph_out$zeMap_background + compromise_graph_out$zeMap_dots + \n        compromise_network_graph_out$zeMap_dots + compromise_network_graph_out$zeMap_text + scatter.theme)\n\n\n\n\n\n\nStep 4: partial factor scores from compromise space\nUsing partial projection, we can now project each condition back onto the compromise space (i.e., partial factor scores) to understand how each correlation matrix fits in the compromise (or sample-level) space.\nTo do that, we should first create a design matrix specifying which slice in our X cube corresponds to which n-back condition:\n\ncondition_design &lt;- vector(\"character\",dim(X)[3])\ncondition_design[grep(\"*_nbk_0b\",dimnames(X)[[3]])]&lt;-\"0b\"\ncondition_design[grep(\"*_nbk_1b\",dimnames(X)[[3]])]&lt;-\"1b\"\ncondition_design[grep(\"*_nbk_2b\",dimnames(X)[[3]])]&lt;-\"2b\"\n\nNow we can compute and plot the partial maps for the different n-back conditions, for each ROI. This plot shows how much the connectivity for each ROI is similar/different across task conditions (closer to the ROI center: more similar; farther from ROI center: more different).\n\n#Compute the Partial map\nF_j     &lt;- covstatis_res$res4Splus$PartialF\nalpha_j &lt;- covstatis_res$res4Cmat$alpha\n# create the groups based on design\ncode4Groups &lt;- unique(condition_design)\n\nnK &lt;- length(code4Groups)\n# initialize F_K and alpha_k\nF_k &lt;- array(0, dim = c(dim(F_j)[[1]], dim(F_j)[[2]],nK))\ndimnames(F_k) &lt;- list(dimnames(F_j)[[1]], \n                      dimnames(F_j)[[2]], code4Groups)\nalpha_k &lt;- rep(0, nK)\nnames(alpha_k) &lt;- code4Groups\nFa_j &lt;- F_j\nfor (j in 1:dim(F_j)[[3]]){ Fa_j[,,j]  &lt;- F_j[,,j] * alpha_j[j] }\nfor (k in 1:nK){\n  lindex &lt;- condition_design == code4Groups[k]\n  alpha_k[k] &lt;- sum(alpha_j[lindex])\n  F_k[,,k] &lt;- (1/alpha_k[k])*apply(Fa_j[,,lindex],c(1,2),sum)\n}\n\ncompromise_pfs &lt;- createPartialFactorScoresMap(\n  factorScores = covstatis_res$res4Splus$F,      \n  partialFactorScores = F_k,  \n  axis1 = 1, axis2 = 2,\n  colors4Items = as.vector(atlas_labels$Network_color), \n  colors4Blocks = c(\"yellowgreen\", \"olivedrab\", \"darkgreen\"),\n  names4Partial = dimnames(F_k)[[3]], # \n  shape.points = 20)\n\nprint(compromise_graph_out$zeMap_background + compromise_graph_out$zeMap_dots +\n  compromise_pfs$mapColByBlocks + ggtitle('Compromise map - Partial factor scores by task design') + scatter.theme)\n\n\n\n\nThat’s a very busy plot! Let’s clean it up to make things easier to interpret. We can compute the partial factor scores by the mean network compromise. Similar to our ROI plot, for each network, the more spread out the conditions are from the network mean, the more diverse the functional connectivity is across task conditions. Tip: if you have groups, sometimes it is helpful to separate partial factor scores per group and plot them individually (e.g., you may not be able to see a difference if everything is plotted together).\n\nF_k_network&lt;-array(NA, dim=c(length(network_labels), dim(F_k)[2], dim(F_k)[3]))\ndimnames(F_k_network)&lt;-list(network_labels, dimnames(F_k)[[2]], dimnames(F_k)[[3]])\n\nfor(i in 1:dim(F_k_network)[3]){\n  F_k_network[,,i]&lt;- t(apply(makeNominalData(as.matrix(atlas_labels$Network)),2,function(x){x/sum(x)})) %*% F_k[,,i]\n}\n\ncompromise_pfs_network &lt;- createPartialFactorScoresMap(\n  factorScores = network_mean_F,      \n  partialFactorScores = F_k_network,  \n  axis1 = 1, axis2 = 2,\n  colors4Items = network_colors, \n  colors4Blocks = c(\"yellowgreen\", \"olivedrab\", \"darkgreen\"),\n  names4Partial = dimnames(F_k_network)[[3]], # \n  size.labels=3.5, size.points = 2,shape.points=20)\n\nprint(compromise_network_graph_out$zeMap + compromise_pfs_network$mapColByBlocks + ggtitle('Network level compromise map - Partial factor scores by task design') + scatter.theme)\n\n\n\n\n\n\nStep 5: area of the convex hull\nWe can now compute the area of the convex hull around the partial factor scores for each individual at the network-level (NB: this can be done at the regional level too). Area of the hull scores can be seen as “task condition differentiation scores”: the larger the area of the hull, the more diverse the network functional connectivity is across task conditions (more distant from mean); the smaller the area of the hull, the more homogenous the functional connectivity across 0-,1-, and 2-back. Note: area of the hull values are arbitrary so you can go ahead and normalize them. Make sure you have at least 4 points to build the area of the hull (here you’ll see warnings cause some networks have less than 4 observations/ROIs).\n\n## Create empty matrix to put results in, should be # participants (rows) by # networks (columns)\ntask_diff_network_level &lt;-data.frame(matrix(0, length(ids),length(network_labels)))\nrownames(task_diff_network_level) &lt;- ids\nnames(task_diff_network_level) &lt;- network_labels\n\npartialF &lt;- covstatis_res$res4Splu$PartialF\n\nfor(i in 1:length(ids)){\n  \n  ### Grab all the partialF values associated with this subject for LV1\n  this_lv1 &lt;- partialF[,1,grep(paste0(ids[i], \"_*\"),dimnames(partialF)[[3]])]\n  ## Transform the node-level partialF to network-level partialF for LV1\n  this_lv1_network&lt;-t(apply(makeNominalData(as.matrix(atlas_labels$Network)),2,\n                            function(x){x/sum(x)})) %*% as.matrix(this_lv1)\n  \n  ### Grab all the partialF values associated with this P for LV2\n  this_lv2 &lt;- partialF[,2,grep(paste0(ids[i], \"_*\"),dimnames(partialF)[[3]])]\n  ## Transform the node-level partialF to network-level partialF for LV2\n  this_lv2_network&lt;-t(apply(makeNominalData(as.matrix(atlas_labels$Network)),2,\n                            function(x){x/sum(x)})) %*% as.matrix(this_lv2)\n  \n  ## Loop through each network, get the LV1 and LV2 points, and compute the area of the hull around the points\n  for(n in 1:length(network_labels)){\n    hull_pts &lt;- chull(this_lv1_network[n,], this_lv2_network[n,]) #here you can play with the flag \"percentage\" and include either all points (like done here) or e.g., 95% of them in which case you'd write \"percentage=0.95\"\n    this_hull_coords &lt;- cbind(this_lv1_network[n,hull_pts],this_lv2_network[n,hull_pts] )\n    this_poly_area &lt;- Polygon(this_hull_coords, hole=F)@area\n    task_diff_network_level[i,n] &lt;- this_poly_area\n    \n  }\n} \n\n#print(task_diff_network_level) #you can go ahead and save this output and relate it to e.g., behavior\n\nArea of the hull scores can be used in secondary analyses, such as brain-behavior correlation. Since our sample is an adult lifespan sample, we can relate area of the hull scores to age. This plot shows how Default A functional connectivity becomes more homogeneous/less differentiated across tasks as a function of age.\n\nsecondary_analyses &lt;- cbind(demog_in$Age, task_diff_network_level)\nsecondary_analyses &lt;- as.data.frame(secondary_analyses)\ncolnames(secondary_analyses)[1] &lt;- \"Age\"\ncolnames(secondary_analyses)[14] &lt;- \"DefaultA\"\nggplot(secondary_analyses, aes(x=Age, y=DefaultA)) + \n  geom_point()+\n  geom_smooth(method=lm)"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "I am finishing up my PhD in Cognitive Neuroscience at the Montreal Neurological Institute at McGill University in Montreal, Canada. I love asking questions about things I don’t understand (all things brain) and trying to find answers that require exploration across desperate research fields. My primary research topic is fMRI BOLD signal variability. I basically investigate what all the variance in brain signals is and why it matters for brain function and organization. Given my approach to science, I would say this topic chose me. Variance in the variance!\nE-mail: giulia.baracchini@mail.mcgill.ca\n\n\n\nI am a post-doctoral research fellow at Centre for Addiction and Mental Health (CAMH). My work focuses on developing advanced multivariate methods to integrate and examine the relationships between structural and functional brain connectivity. Prior to joining CAMH, I received my Ph.D. in Cognition & Neuroscience from the University of Texas at Dallas, where she focused on developing advanced multivariate methods (with sparsification and for novel applications) to analyze neuroscience data (including behavioral data, genetics data, fMRI data, and resting-state fMRI data).\nE-mail: Ju-Chi.Yu@camh.ca"
  },
  {
    "objectID": "pages/about.html#tutorial-manuscript",
    "href": "pages/about.html#tutorial-manuscript",
    "title": "About",
    "section": "",
    "text": "I am finishing up my PhD in Cognitive Neuroscience at the Montreal Neurological Institute at McGill University in Montreal, Canada. I love asking questions about things I don’t understand (all things brain) and trying to find answers that require exploration across desperate research fields. My primary research topic is fMRI BOLD signal variability. I basically investigate what all the variance in brain signals is and why it matters for brain function and organization. Given my approach to science, I would say this topic chose me. Variance in the variance!\nE-mail: giulia.baracchini@mail.mcgill.ca\n\n\n\nI am a post-doctoral research fellow at Centre for Addiction and Mental Health (CAMH). My work focuses on developing advanced multivariate methods to integrate and examine the relationships between structural and functional brain connectivity. Prior to joining CAMH, I received my Ph.D. in Cognition & Neuroscience from the University of Texas at Dallas, where she focused on developing advanced multivariate methods (with sparsification and for novel applications) to analyze neuroscience data (including behavioral data, genetics data, fMRI data, and resting-state fMRI data).\nE-mail: Ju-Chi.Yu@camh.ca"
  },
  {
    "objectID": "pages/about.html#covstatis-source-code",
    "href": "pages/about.html#covstatis-source-code",
    "title": "About",
    "section": "covSTATIS source code:",
    "text": "covSTATIS source code:\n\nDistatisR package\n\nCRAN - by Hervé Abdi & Derek Beaton\n\nVersion: 1.1.1\nPublished: 2022-12-05\nhelp file\n\nVersion under development - by Hervé Abdi, Derek Beaton, Vincent Guillemot & Ju-Chi Yu"
  },
  {
    "objectID": "pages/about.html#credit-statement-for-the-paper",
    "href": "pages/about.html#credit-statement-for-the-paper",
    "title": "About",
    "section": "Credit statement for the paper:",
    "text": "Credit statement for the paper:\n\nGiulia Baracchini: Conceptualization, Software, Formal Analysis, Data Curation, Writing - Original Draft, Writing - Review and Editing, Visualization.\nJu-Chi Yu: Conceptualization, Software, Formal Analysis, Writing - Original Draft, Writing - Review and Editing.\nJenny Rieck: Software, Formal Analysis, Data Curation, Writing - Review and Editing.\nDerek Beaton: Software, Writing - Review and Editing.\nVincent Guillemot: Software, Writing - Review and Editing.\nCheryl Grady: Resources, Writing - Review and Editing.\nHervé Abdi: Software, Writing - Review and Editing.\nR. Nathan Spreng: Writing - Review and Editing, Supervision."
  },
  {
    "objectID": "pages/about.html#website-credit",
    "href": "pages/about.html#website-credit",
    "title": "About",
    "section": "Website credit",
    "text": "Website credit\n\nVincent Guillemot"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible example for the paper “covSTATIS: a multi-table technique for network neuroscience”",
    "section": "",
    "text": "This website provides the tutorial with reproducible example for our manuscript “covSTATIS: a multi-table technique for network neuroscience”.\nIn this repo, you’ll find data (/Data) and code (/pages/tutorial.qmd) we used to create our covSTATIS tutorial. All info on how to run the tutorial yourself is in the .qmd file (or as .rmd file) :)\nHappy covSTATISing :) PS If you have any questions, don’t hesitate to reach out!"
  },
  {
    "objectID": "pages/extensions.html",
    "href": "pages/extensions.html",
    "title": "Other extensions",
    "section": "",
    "text": "Here are some extension techniques that are under development:\n\nSparse covSTATIS by Ju-Chi Yu, Ph.D.\n\nCode for the paper\nThe SPAFAC Package - under testing for sparse covSTATIS\n\nK+1 covSTATIS by Derek Beaton, Ph.D. & Jenny Rieck, Ph.D.\nDiscriminant and hierarchical DiSTATIS by Michael Kreigsman, Ph.D.\nSTATIS method for individualized parcels and network organizations by Ju-Chi Yu, Ph.D. & Micaela Chan, Ph.D."
  }
]